
@with_kw struct ValueIteration <: DecisionAlgorithm
    max_iterations::Int = 100
end

# There is no update_hyperparameters!(...).

function ValueIteration(prob::DecisionNetwork)
    reward_space = support(prob[:r])
    state_space = support(prob[:sp])

    # Finite and discrete only
    states = [s for s in state_space]

    discountfactor = 0.1

    V = zeros(eltype(reward_space), length(states))
    random_action_dist = UniformDist(support(prob[:a]))
    Ï€ = [random_action_dist() for s in state_space]

    for _ in 1:1000
        for s in state_space
            Vs_best = zero(reward_space)
            a_best = random_action_dist()

            for a in ğ’œ(Î .ğ’«)
                Vs = zero(reward_space)
                for sp in support(prob[:sp])
                    Vs += prob[:r](; s, a, sp)
                    Vs += discountfactor * prob[:sp](sp; s, a) * V[sp]
                end

                if Vs_best < Vs
                    Vs_best, a_best = Vs, a
                end
            end

            # Actually update / store Vs
        end
    end

end


function update_parameters(agent::DecisionAgent, behavior::ConditionalDist)

end

function behavior(::DecisionNetwork, ::DecisionAgent) end

function DecisionMaking.initialvars!(Î ::VI)

    râ‚€ = similar(first(â„›(Î .ğ’«)))
    for s in ğ’®(Î .ğ’«)
        Î .vars.V[s] = similar(râ‚€)
        Î .vars.Ï€[s] = rand(Î .rng, ğ’œ(Î .ğ’«))
    end

    V(s) = begin
        try return isterminal(Î .ğ’«, s) ? similar(râ‚€) : Î .vars.V[s]
        catch e return Î .vars.V[s] end
    end

    # NOTE: If there happen to be multiple agents and/or multiple factors,
    # then this VI code below employs a Bellman operator for which its
    # "max_a" operator requires a strict maximum over all agents and/or
    # factors. Ties are broken by randomly (implicitly in code).

    for i in 1:Î .hparams.max_iterations
        for s in ğ’®(Î .ğ’«)
            Vs_best, a_best = similar(râ‚€; value=-Inf), rand(Î .rng, ğ’œ(Î .ğ’«))

            for a in ğ’œ(Î .ğ’«)
                Vs = similar(râ‚€)
                Td = T(Î .ğ’«, s, a)
                for sâ€² in support(Td)
                    Vs += R(Î .ğ’«, s, a, sâ€²; r=nothing)
                    Vs += discountfactor(Î .ğ’«, Î .ğ’¥) * T(Î .ğ’«, s, a; sâ€²=sâ€²) * V(sâ€²)
                end

                if Vs_best < Vs
                    Vs_best, a_best = Vs, a
                end
            end

            Î .vars.V[s], Î .vars.Ï€[s] = Vs_best, a_best
        end
    end
end


# =====

@with_kw mutable struct VIHyperparameters <: DMAlgorithmHyperparameters
    max_iterations::Int = 100
end

mutable struct VIVariables <: DMAlgorithmVariables
    V::Dict{DMState, DMReward}
    Ï€::Dict{DMState, DMAction}

    VIVariables(ğ’«::DMProblem) = new(
        Dict(s => similar(first(â„›(ğ’«))) for s in ğ’®(ğ’«)),
        Dict(s => first(ğ’œ(ğ’«)) for s in ğ’®(ğ’«)),
    )
end

struct VI <: DMAlgorithm
    hparams::VIHyperparameters
    vars::VIVariables

    ğ’«::DMProblem
    ğ’¥::DMObjective
    rng::AbstractRNG

    VI(ğ’«::DMProblem;
        ğ’¥::DMObjective=DMInfiniteHorizonObjective(),
        rng::AbstractRNG=Xoshiro(),
        max_iterations::Int=1,
    ) = new(VIHyperparameters(max_iterations), VIVariables(ğ’«), ğ’«, ğ’¥, rng)
end

DecisionMaking.â„‹(Î ::VI) = DMIterator{VIHyperparameters}(0:100:1000)

function DecisionMaking.initialvars!(Î ::VI)
    râ‚€ = similar(first(â„›(Î .ğ’«)))
    for s in ğ’®(Î .ğ’«)
        Î .vars.V[s] = similar(râ‚€)
        Î .vars.Ï€[s] = rand(Î .rng, ğ’œ(Î .ğ’«))
    end

    V(s) = begin
        try return isterminal(Î .ğ’«, s) ? similar(râ‚€) : Î .vars.V[s]
        catch e return Î .vars.V[s] end
    end

    # NOTE: If there happen to be multiple agents and/or multiple factors,
    # then this VI code below employs a Bellman operator for which its
    # "max_a" operator requires a strict maximum over all agents and/or
    # factors. Ties are broken by randomly (implicitly in code).

    for i in 1:Î .hparams.max_iterations
        for s in ğ’®(Î .ğ’«)
            Vs_best, a_best = similar(râ‚€; value=-Inf), rand(Î .rng, ğ’œ(Î .ğ’«))

            for a in ğ’œ(Î .ğ’«)
                Vs = similar(râ‚€)
                Td = T(Î .ğ’«, s, a)
                for sâ€² in support(Td)
                    Vs += R(Î .ğ’«, s, a, sâ€²; r=nothing)
                    Vs += discountfactor(Î .ğ’«, Î .ğ’¥) * T(Î .ğ’«, s, a; sâ€²=sâ€²) * V(sâ€²)
                end

                if Vs_best < Vs
                    Vs_best, a_best = Vs, a
                end
            end

            Î .vars.V[s], Î .vars.Ï€[s] = Vs_best, a_best
        end
    end
end

function DecisionMaking.Ïˆ(Î ::VI, s::DMState;
        a::DMActionOrMissing=missing,
        agent::DMIndexOrMissing=missing, factor::DMIndexOrMissing=missing,
    )
    return ismissing(a) ? DMDeterministicDistribution(Î .vars.Ï€[s]) : 1.0 * (Î .vars.Ï€[s] == a)
end

DecisionMaking.iscountablyinfinite(Î ::VI, t::Type{<:DMAction})::Bool = false
DecisionMaking.iscontinuous(Î ::VI, t::Type{<:DMAction})::Bool = false
DecisionMaking.isdeterministic(Î ::VI, t::Type{<:DMAction})::Bool = true

#DecisionMaking.order(Î ::VI, T::Type{<:DMAction}) # Use Default. It works well automatically.
